{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoderModel(nn.Module):\n",
    "    def __init__(self, latent_dim=10, hidden_dim=64):\n",
    "        super(VariationalAutoEncoderModel, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, hidden_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flattened_dim = hidden_dim * 7 * 7\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        \n",
    "        self.decoder_input = nn.Linear(latent_dim, self.flattened_dim)\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim, 32, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "                \n",
    "            nn.ConvTranspose2d(16, 1, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(x.size(0), self.hidden_dim, 7, 7) \n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def elbo_loss(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def sample(self, num_samples, current_device):\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder:\n",
    "    def __init__(self, loader, epochs=50, learning_rate=0.001, input_size=28):\n",
    "        pass\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.model = VariationalAutoEncoderModel()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_function = self.model.elbo_loss\n",
    "        self.loader = loader\n",
    "        \n",
    "    def train(self):\n",
    "        outputs = []\n",
    "        self.losses = []\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            with tqdm(self.loader, desc=f\"Epoch {epoch+1}/{self.epochs}\") as t:\n",
    "                for images, _ in t:\n",
    "                    images = images.view(images.size(0), -1, self.input_size, self.input_size).to(device)\n",
    "                    \n",
    "                    #def add_noise(image, noise_factor=0.3):\n",
    "                    #    noise = torch.randn_like(image) * noise_factor\n",
    "                    #    noisy_image = image + noise\n",
    "                    #    return torch.clamp(noisy_image, 0., 1.)  \n",
    "                    #noisy_images = add_noise(images)\n",
    "\n",
    "                    # Unpack the model's outputs\n",
    "                    reconstructed, mu, logvar = self.model(images)\n",
    "\n",
    "                    # Compute the loss\n",
    "                    loss = self.loss_function(reconstructed, images, mu, logvar)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    self.losses.append(loss.item())\n",
    "                    \n",
    "                    t.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "\n",
    "            outputs.append((epoch, images, reconstructed))\n",
    "        return outputs, self.losses\n",
    "    \n",
    "    def show_latent_variation(self, images, device, num_images=6, shift_amount=1.0):\n",
    "        self.model.eval()\n",
    "        images = images[:num_images].to(device)\n",
    "\n",
    "        # Encode the images\n",
    "        mu, logvar = self.model.encode(images)\n",
    "        z_original = self.model.reparameterize(mu, logvar)\n",
    "\n",
    "        # Shift the latent space slightly\n",
    "        z_shifted = z_original + torch.randn_like(z_original) * shift_amount\n",
    "\n",
    "        # Decode both original and shifted latent vectors\n",
    "        recon_original = self.model.decode(z_original)\n",
    "        recon_shifted = self.model.decode(z_shifted)\n",
    "\n",
    "        # Convert to NumPy for visualization\n",
    "        images = images.cpu().detach().numpy()\n",
    "        recon_original = recon_original.cpu().detach().numpy()\n",
    "        recon_shifted = recon_shifted.cpu().detach().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(3, num_images, figsize=(12, 6))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            axes[0, i].imshow(images[i].squeeze(), cmap=\"gray\")\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "            axes[1, i].imshow(recon_original[i].squeeze(), cmap=\"gray\")\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "            axes[2, i].imshow(recon_shifted[i].squeeze(), cmap=\"gray\")\n",
    "            axes[2, i].axis('off')\n",
    "\n",
    "        axes[0, 0].set_title(\"Original Images\")\n",
    "        axes[1, 0].set_title(\"Reconstructed\")\n",
    "        axes[2, 0].set_title(\"Latent Shifted\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.figure(figsize=(4, 2))\n",
    "        plt.plot(self.losses[1:], label='Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  22%|██▏       | 210/938 [00:03<00:12, 59.16it/s, loss=8304.990234] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m loader = torch.utils.data.DataLoader(\n\u001b[32m      2\u001b[39m             datasets.MNIST(\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mTrue\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      3\u001b[39m                            transform=transforms.Compose([\n\u001b[32m      4\u001b[39m                                transforms.ToTensor()\n\u001b[32m      5\u001b[39m                            ])),\n\u001b[32m      6\u001b[39m             batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m mnist_vae = VariationalAutoEncoder(loader)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m outputs, losses = \u001b[43mmnist_vae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m mnist_vae.plot_loss()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mVariationalAutoEncoder.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m.loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#def add_noise(image, noise_factor=0.3):\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#    noise = torch.randn_like(image) * noise_factor\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#    noisy_image = image + noise\u001b[39;49;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Unpack the model's outputs\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('../data', train=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor()\n",
    "                           ])),\n",
    "            batch_size=64, shuffle=True)\n",
    "mnist_vae = VariationalAutoEncoder(loader)\n",
    "\n",
    "outputs, losses = mnist_vae.train()\n",
    "mnist_vae.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mnist_vae.show_latent_variation( \u001b[43moutputs\u001b[49m[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m], device, num_images=\u001b[32m6\u001b[39m, shift_amount=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mnist_vae.show_latent_variation( outputs[-1][1], device, num_images=6, shift_amount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERecModel(nn.Module):\n",
    "    def __init__(self, num_items, latent_dim=10, hidden_dim=64, dropout=0.2):\n",
    "        super(VAERecModel, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_items, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)  # Output both mu and logvar\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(latent_dim * 2, latent_dim)\n",
    "        self.fc_var = nn.Linear(latent_dim * 2, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, num_items),\n",
    "            nn.Sigmoid()  # Output is between 0 and 1 (probability of liking an item)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def elbo_loss(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def sample(self, num_samples, current_device):\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderDataset(data.Dataset):\n",
    "    \"\"\"Custom Dataset to handle User-Item Interactions\"\"\"\n",
    "    def __init__(self, user_item_matrix):\n",
    "        self.data = torch.FloatTensor(user_item_matrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERecommenderSystem:\n",
    "    def __init__(self, user_item_matrix, latent_dim=50, hidden_dim=200, batch_size=64, lr=0.001, epochs=20, val_split=0.2):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_items = user_item_matrix.shape[1]\n",
    "        self.model = VAERecModel(self.num_items, latent_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        train_data, val_data = train_test_split(user_item_matrix, test_size=val_split, random_state=42)\n",
    "\n",
    "        # Prepare dataset loaders\n",
    "        self.train_dataset = RecommenderDataset(train_data)\n",
    "        self.val_dataset = RecommenderDataset(val_data)\n",
    "\n",
    "        self.train_loader = data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Store loss history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training started...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in self.train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                recon_batch, mu, logvar = self.model(batch)\n",
    "                loss = self.model.elbo_loss(recon_batch, batch, mu, logvar)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(self.train_loader)\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation Step\n",
    "            val_loss = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} - Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                recon_batch, mu, logvar = self.model(batch)\n",
    "                loss = self.model.elbo_loss(recon_batch, batch, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        return val_loss / len(self.val_loader)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.train_losses, label=\"Train Loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, user_vector):\n",
    "        \"\"\"Given a user's interaction vector, return top-K recommended items.\"\"\"\n",
    "        self.model.eval()\n",
    "        user_vector = torch.FloatTensor(user_vector).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.model.encode(user_vector)\n",
    "            z = self.model.reparameterize(mu, logvar)\n",
    "            predictions = self.model.decode(z)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def recommend_items(self, user_data, user_idx=None, top_k=10, exclude_known=True):\n",
    "        if user_idx is not None:\n",
    "            user_vector = user_data[user_idx].unsqueeze(0)\n",
    "        else:\n",
    "            user_vector = user_data\n",
    "            \n",
    "        predictions = self.predict(user_vector).squeeze().cpu().numpy()\n",
    "        \n",
    "        if exclude_known and user_idx is not None:\n",
    "            known_items = user_data[user_idx].cpu().numpy() > 0\n",
    "            predictions[known_items] = -float('inf')\n",
    "        \n",
    "        top_items = np.argsort(predictions)[::-1][:top_k]\n",
    "        return [(item_idx, predictions[item_idx]) for item_idx in top_items]\n",
    "        \n",
    "    def latent_space_variation(self, user_vector, shift_amount=1.0):\n",
    "        \"\"\"Shift latent space and see how recommendations change\"\"\"\n",
    "        self.model.eval()\n",
    "        user_vector = torch.FloatTensor(user_vector).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.model.encode(user_vector)\n",
    "            z_original = self.model.reparameterize(mu, logvar)\n",
    "            z_shifted = z_original + torch.randn_like(z_original) * shift_amount\n",
    "\n",
    "            recon_original = self.model.decode(z_original).cpu().detach().numpy().flatten()\n",
    "            recon_shifted = self.model.decode(z_shifted).cpu().detach().numpy().flatten()\n",
    "\n",
    "        return np.argsort(recon_original)[::-1][:5], np.argsort(recon_shifted)[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "df = pd.read_csv('data/lfm_interactions.csv', sep=\"\\t\", index_col=0)\n",
    "inter_matr = pd.pivot_table(df, values='count', index='user_id', columns='item_id')\n",
    "inter_matr = inter_matr.fillna(0).to_numpy()\n",
    "inter_matr = (inter_matr > 0).astype(int)\n",
    "\n",
    "inter_tensor = torch.FloatTensor(inter_matr)\n",
    "train_data, test_data = train_test_split(inter_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 1/56 - Train Loss: 68126.7967 | Val Loss: 13555.0756\n",
      "Epoch 2/56 - Train Loss: 12863.0504 | Val Loss: 13025.8697\n",
      "Epoch 3/56 - Train Loss: 12656.7618 | Val Loss: 13007.7227\n",
      "Epoch 4/56 - Train Loss: 12566.7131 | Val Loss: 12907.7016\n",
      "Epoch 5/56 - Train Loss: 12543.2787 | Val Loss: 12783.4516\n",
      "Epoch 6/56 - Train Loss: 12427.0464 | Val Loss: 12444.5549\n",
      "Epoch 7/56 - Train Loss: 12185.7814 | Val Loss: 12411.1738\n",
      "Epoch 8/56 - Train Loss: 12171.2561 | Val Loss: 12345.2586\n",
      "Epoch 9/56 - Train Loss: 12168.5842 | Val Loss: 12342.4881\n",
      "Epoch 10/56 - Train Loss: 12164.8200 | Val Loss: 12330.8571\n",
      "Epoch 11/56 - Train Loss: 12129.1699 | Val Loss: 12331.9140\n",
      "Epoch 12/56 - Train Loss: 12125.1825 | Val Loss: 12312.6229\n",
      "Epoch 13/56 - Train Loss: 12105.1470 | Val Loss: 12391.1761\n",
      "Epoch 14/56 - Train Loss: 12111.7332 | Val Loss: 12268.8912\n",
      "Epoch 15/56 - Train Loss: 12098.6474 | Val Loss: 12257.4937\n",
      "Epoch 16/56 - Train Loss: 12094.9849 | Val Loss: 12354.6337\n",
      "Epoch 17/56 - Train Loss: 12089.5564 | Val Loss: 12329.2377\n",
      "Epoch 18/56 - Train Loss: 12071.0440 | Val Loss: 12263.2581\n",
      "Epoch 19/56 - Train Loss: 12073.2773 | Val Loss: 12270.9748\n",
      "Epoch 20/56 - Train Loss: 12060.3787 | Val Loss: 12290.7819\n",
      "Epoch 21/56 - Train Loss: 12048.5355 | Val Loss: 12314.2123\n",
      "Epoch 22/56 - Train Loss: 12036.3132 | Val Loss: 12344.4697\n",
      "Epoch 23/56 - Train Loss: 12020.8029 | Val Loss: 12316.0061\n",
      "Epoch 24/56 - Train Loss: 12005.8208 | Val Loss: 12261.0532\n",
      "Epoch 25/56 - Train Loss: 12001.7714 | Val Loss: 12340.1961\n",
      "Epoch 26/56 - Train Loss: 11981.0848 | Val Loss: 12300.6220\n",
      "Epoch 27/56 - Train Loss: 11947.1586 | Val Loss: 12355.8549\n",
      "Epoch 28/56 - Train Loss: 11913.6319 | Val Loss: 12233.1866\n",
      "Epoch 29/56 - Train Loss: 11864.5381 | Val Loss: 12282.8675\n",
      "Epoch 30/56 - Train Loss: 11738.1759 | Val Loss: 12059.5323\n",
      "Epoch 31/56 - Train Loss: 11596.0387 | Val Loss: 11916.3355\n",
      "Epoch 32/56 - Train Loss: 11419.8462 | Val Loss: 11747.6900\n",
      "Epoch 33/56 - Train Loss: 11265.3424 | Val Loss: 11690.0248\n",
      "Epoch 34/56 - Train Loss: 11260.6103 | Val Loss: 11651.6597\n",
      "Epoch 35/56 - Train Loss: 11127.9921 | Val Loss: 11536.2288\n",
      "Epoch 36/56 - Train Loss: 11079.9556 | Val Loss: 11563.8605\n",
      "Epoch 37/56 - Train Loss: 11003.7611 | Val Loss: 11470.7751\n",
      "Epoch 38/56 - Train Loss: 10951.2334 | Val Loss: 11373.6855\n",
      "Epoch 39/56 - Train Loss: 10818.9455 | Val Loss: 11387.4514\n",
      "Epoch 40/56 - Train Loss: 10745.7950 | Val Loss: 11337.0406\n",
      "Epoch 41/56 - Train Loss: 10599.7841 | Val Loss: 11293.6151\n",
      "Epoch 42/56 - Train Loss: 10485.8422 | Val Loss: 11081.3513\n",
      "Epoch 43/56 - Train Loss: 10341.0575 | Val Loss: 11095.0755\n",
      "Epoch 44/56 - Train Loss: 10270.3599 | Val Loss: 11027.7748\n",
      "Epoch 45/56 - Train Loss: 10191.7012 | Val Loss: 10906.8247\n",
      "Epoch 46/56 - Train Loss: 10081.9546 | Val Loss: 10950.3275\n",
      "Epoch 47/56 - Train Loss: 9981.9345 | Val Loss: 10953.9944\n",
      "Epoch 48/56 - Train Loss: 9921.7945 | Val Loss: 10914.2764\n",
      "Epoch 49/56 - Train Loss: 9835.0128 | Val Loss: 10840.3466\n",
      "Epoch 50/56 - Train Loss: 9761.9307 | Val Loss: 10866.4080\n",
      "Epoch 51/56 - Train Loss: 9689.7483 | Val Loss: 10795.8309\n",
      "Epoch 52/56 - Train Loss: 9627.6700 | Val Loss: 10790.5142\n",
      "Epoch 53/56 - Train Loss: 9556.0940 | Val Loss: 10709.1347\n",
      "Epoch 54/56 - Train Loss: 9491.3473 | Val Loss: 10724.0877\n",
      "Epoch 55/56 - Train Loss: 9394.5456 | Val Loss: 10766.1230\n",
      "Epoch 56/56 - Train Loss: 9338.4651 | Val Loss: 10638.5734\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_recommender = VAERecommenderSystem(train_data, latent_dim=64, epochs=56, val_split=0.25)\n",
    "vae_recommender.train()\n",
    "vae_recommender.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('test_users.txt')\n",
    "user_recommendations = []\n",
    "\n",
    "for id in ids.user_id:\n",
    "    recommendation = vae_recommender.recommend_items(inter_tensor, id, top_k=10)\n",
    "    item_ids = [str(item_id) for item_id, _ in recommendation]\n",
    "    user_recommendations.append([id, ','.join(item_ids)])\n",
    "\n",
    "df = pd.DataFrame(user_recommendations, columns=['user_id', 'recommendations'])\n",
    "\n",
    "df.to_csv('res/rec_vae.tsv', index=False, sep='\\t', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
