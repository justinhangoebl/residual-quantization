{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoderModel(nn.Module):\n",
    "    def __init__(self, latent_dim=10, hidden_dim=64):\n",
    "        super(VariationalAutoEncoderModel, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, hidden_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flattened_dim = hidden_dim * 7 * 7\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        \n",
    "        self.decoder_input = nn.Linear(latent_dim, self.flattened_dim)\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim, 32, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "                \n",
    "            nn.ConvTranspose2d(16, 1, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(x.size(0), self.hidden_dim, 7, 7) \n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def elbo_loss(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def sample(self, num_samples, current_device):\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder:\n",
    "    def __init__(self, loader, epochs=50, learning_rate=0.001, input_size=28):\n",
    "        pass\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.model = VariationalAutoEncoderModel()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_function = self.model.elbo_loss\n",
    "        self.loader = loader\n",
    "        \n",
    "    def train(self):\n",
    "        outputs = []\n",
    "        self.losses = []\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            with tqdm(self.loader, desc=f\"Epoch {epoch+1}/{self.epochs}\") as t:\n",
    "                for images, _ in t:\n",
    "                    images = images.view(images.size(0), -1, self.input_size, self.input_size).to(device)\n",
    "                    \n",
    "                    #def add_noise(image, noise_factor=0.3):\n",
    "                    #    noise = torch.randn_like(image) * noise_factor\n",
    "                    #    noisy_image = image + noise\n",
    "                    #    return torch.clamp(noisy_image, 0., 1.)  \n",
    "                    #noisy_images = add_noise(images)\n",
    "\n",
    "                    # Unpack the model's outputs\n",
    "                    reconstructed, mu, logvar = self.model(images)\n",
    "\n",
    "                    # Compute the loss\n",
    "                    loss = self.loss_function(reconstructed, images, mu, logvar)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    self.losses.append(loss.item())\n",
    "                    \n",
    "                    t.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "\n",
    "            outputs.append((epoch, images, reconstructed))\n",
    "        return outputs, self.losses\n",
    "    \n",
    "    def show_latent_variation(self, images, device, num_images=6, shift_amount=1.0):\n",
    "        self.model.eval()\n",
    "        images = images[:num_images].to(device)\n",
    "\n",
    "        # Encode the images\n",
    "        mu, logvar = self.model.encode(images)\n",
    "        z_original = self.model.reparameterize(mu, logvar)\n",
    "\n",
    "        # Shift the latent space slightly\n",
    "        z_shifted = z_original + torch.randn_like(z_original) * shift_amount\n",
    "\n",
    "        # Decode both original and shifted latent vectors\n",
    "        recon_original = self.model.decode(z_original)\n",
    "        recon_shifted = self.model.decode(z_shifted)\n",
    "\n",
    "        # Convert to NumPy for visualization\n",
    "        images = images.cpu().detach().numpy()\n",
    "        recon_original = recon_original.cpu().detach().numpy()\n",
    "        recon_shifted = recon_shifted.cpu().detach().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(3, num_images, figsize=(12, 6))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            axes[0, i].imshow(images[i].squeeze(), cmap=\"gray\")\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "            axes[1, i].imshow(recon_original[i].squeeze(), cmap=\"gray\")\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "            axes[2, i].imshow(recon_shifted[i].squeeze(), cmap=\"gray\")\n",
    "            axes[2, i].axis('off')\n",
    "\n",
    "        axes[0, 0].set_title(\"Original Images\")\n",
    "        axes[1, 0].set_title(\"Reconstructed\")\n",
    "        axes[2, 0].set_title(\"Latent Shifted\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.figure(figsize=(4, 2))\n",
    "        plt.plot(self.losses[1:], label='Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  54%|█████▍    | 511/938 [00:07<00:06, 67.69it/s, loss=6725.611328] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m loader = torch.utils.data.DataLoader(\n\u001b[32m      2\u001b[39m             datasets.MNIST(\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mTrue\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      3\u001b[39m                            transform=transforms.Compose([\n\u001b[32m      4\u001b[39m                                transforms.ToTensor()\n\u001b[32m      5\u001b[39m                            ])),\n\u001b[32m      6\u001b[39m             batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m mnist_vae = VariationalAutoEncoder(loader)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m outputs, losses = \u001b[43mmnist_vae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m mnist_vae.plot_loss()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mVariationalAutoEncoder.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m     37\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.losses.append(loss.item())\n\u001b[32m     42\u001b[39m t.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:234\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    231\u001b[39m     state_steps: List[Tensor] = []\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m     adam(\n\u001b[32m    245\u001b[39m         params_with_grad,\n\u001b[32m    246\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m         found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    265\u001b[39m     )\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Justin\\Documents\\Master-Thesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:148\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         has_complex |= \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m         params_with_grad.append(p)\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.is_sparse:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('../data', train=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor()\n",
    "                           ])),\n",
    "            batch_size=64, shuffle=True)\n",
    "mnist_vae = VariationalAutoEncoder(loader)\n",
    "\n",
    "outputs, losses = mnist_vae.train()\n",
    "mnist_vae.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mnist_vae.show_latent_variation( \u001b[43moutputs\u001b[49m[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m], device, num_images=\u001b[32m6\u001b[39m, shift_amount=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mnist_vae.show_latent_variation( outputs[-1][1], device, num_images=6, shift_amount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERecModel(nn.Module):\n",
    "    def __init__(self, num_items, latent_dim=10, hidden_dim=64, dropout=0.2):\n",
    "        super(VAERecModel, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_items, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)  # Output both mu and logvar\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(latent_dim * 2, latent_dim)\n",
    "        self.fc_var = nn.Linear(latent_dim * 2, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, num_items),\n",
    "            nn.Sigmoid()  # Output is between 0 and 1 (probability of liking an item)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def elbo_loss(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def sample(self, num_samples, current_device):\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderDataset(data.Dataset):\n",
    "    \"\"\"Custom Dataset to handle User-Item Interactions\"\"\"\n",
    "    def __init__(self, user_item_matrix):\n",
    "        self.data = torch.FloatTensor(user_item_matrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERecommenderSystem:\n",
    "    def __init__(self, user_item_matrix, latent_dim=64, hidden_dim=256, batch_size=1, lr=0.001, epochs=56, val_split=0.2):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_items = user_item_matrix.shape[1]\n",
    "        self.model = VAERecModel(self.num_items, latent_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        train_data, val_data = train_test_split(user_item_matrix, test_size=val_split, random_state=42)\n",
    "\n",
    "        # Prepare dataset loaders\n",
    "        self.train_dataset = RecommenderDataset(train_data)\n",
    "        self.val_dataset = RecommenderDataset(val_data)\n",
    "\n",
    "        self.train_loader = data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Store loss history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training started...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in self.train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                recon_batch, mu, logvar = self.model(batch)\n",
    "                loss = self.model.elbo_loss(recon_batch, batch, mu, logvar)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(self.train_loader)\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation Step\n",
    "            val_loss = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} - Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                recon_batch, mu, logvar = self.model(batch)\n",
    "                loss = self.model.elbo_loss(recon_batch, batch, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        return val_loss / len(self.val_loader)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.train_losses, label=\"Train Loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, user_vector):\n",
    "        \"\"\"Given a user's interaction vector, return top-K recommended items.\"\"\"\n",
    "        self.model.eval()\n",
    "        user_vector = torch.FloatTensor(user_vector).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.model.encode(user_vector)\n",
    "            z = self.model.reparameterize(mu, logvar)\n",
    "            predictions = self.model.decode(z)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def recommend_items(self, user_data, user_idx=None, top_k=10, exclude_known=True):\n",
    "        if user_idx is not None:\n",
    "            user_vector = user_data[user_idx].unsqueeze(0)\n",
    "        else:\n",
    "            user_vector = user_data\n",
    "            \n",
    "        predictions = self.predict(user_vector).squeeze().cpu().numpy()\n",
    "        \n",
    "        if exclude_known and user_idx is not None:\n",
    "            known_items = user_data[user_idx].cpu().numpy() > 0\n",
    "            predictions[known_items] = -float('inf')\n",
    "        \n",
    "        top_items = np.argsort(predictions)[::-1][:top_k]\n",
    "        return [(item_idx, predictions[item_idx]) for item_idx in top_items]\n",
    "        \n",
    "    def latent_space_variation(self, user_vector, shift_amount=1.0):\n",
    "        \"\"\"Shift latent space and see how recommendations change\"\"\"\n",
    "        self.model.eval()\n",
    "        user_vector = torch.FloatTensor(user_vector).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.model.encode(user_vector)\n",
    "            z_original = self.model.reparameterize(mu, logvar)\n",
    "            z_shifted = z_original + torch.randn_like(z_original) * shift_amount\n",
    "\n",
    "            recon_original = self.model.decode(z_original).cpu().detach().numpy().flatten()\n",
    "            recon_shifted = self.model.decode(z_shifted).cpu().detach().numpy().flatten()\n",
    "\n",
    "        return np.argsort(recon_original)[::-1][:5], np.argsort(recon_shifted)[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "df = pd.read_csv('data/lfm_interactions.csv', sep=\"\\t\", index_col=0)\n",
    "inter_matr = pd.pivot_table(df, values='count', index='user_id', columns='item_id')\n",
    "inter_matr = inter_matr.fillna(0).to_numpy()\n",
    "inter_matr = (inter_matr > 0).astype(int)\n",
    "\n",
    "inter_tensor = torch.FloatTensor(inter_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 1/56 - Train Loss: 228.5692 | Val Loss: 212.0929\n",
      "Epoch 2/56 - Train Loss: 203.7279 | Val Loss: 199.2943\n",
      "Epoch 3/56 - Train Loss: 201.0222 | Val Loss: 193.0032\n",
      "Epoch 4/56 - Train Loss: 197.8888 | Val Loss: 192.2014\n",
      "Epoch 5/56 - Train Loss: 195.9290 | Val Loss: 190.4891\n",
      "Epoch 6/56 - Train Loss: 194.9237 | Val Loss: 190.5222\n",
      "Epoch 7/56 - Train Loss: 194.5668 | Val Loss: 191.9749\n",
      "Epoch 8/56 - Train Loss: 193.5858 | Val Loss: 188.8634\n",
      "Epoch 9/56 - Train Loss: 189.2968 | Val Loss: 183.1795\n",
      "Epoch 10/56 - Train Loss: 190.4883 | Val Loss: 181.6848\n",
      "Epoch 11/56 - Train Loss: 182.7434 | Val Loss: 181.5257\n",
      "Epoch 12/56 - Train Loss: 193.2898 | Val Loss: 182.2875\n",
      "Epoch 13/56 - Train Loss: 179.2269 | Val Loss: 178.4523\n",
      "Epoch 14/56 - Train Loss: 200.2366 | Val Loss: 175.2343\n",
      "Epoch 15/56 - Train Loss: 55827.2095 | Val Loss: 173.3604\n",
      "Epoch 16/56 - Train Loss: 191.8128 | Val Loss: 173.0525\n",
      "Epoch 17/56 - Train Loss: 172.8743 | Val Loss: 172.1817\n",
      "Epoch 18/56 - Train Loss: 172.4470 | Val Loss: 172.4344\n",
      "Epoch 19/56 - Train Loss: 167.9790 | Val Loss: 169.9316\n",
      "Epoch 20/56 - Train Loss: 1815071933.2831 | Val Loss: 172.1354\n",
      "Epoch 21/56 - Train Loss: 165.7085 | Val Loss: 172.8409\n",
      "Epoch 22/56 - Train Loss: 172.5695 | Val Loss: 170.7590\n",
      "Epoch 23/56 - Train Loss: 21681.1617 | Val Loss: 171.7657\n",
      "Epoch 24/56 - Train Loss: 169.2457 | Val Loss: 169.6855\n",
      "Epoch 25/56 - Train Loss: 160.9006 | Val Loss: 169.7069\n",
      "Epoch 26/56 - Train Loss: 161.0785 | Val Loss: 169.4128\n",
      "Epoch 27/56 - Train Loss: 167.1930 | Val Loss: 171.9056\n",
      "Epoch 28/56 - Train Loss: 1894.8912 | Val Loss: 174.4765\n",
      "Epoch 29/56 - Train Loss: 161.0460 | Val Loss: 169.3318\n",
      "Epoch 30/56 - Train Loss: 166.8187 | Val Loss: 172.8728\n",
      "Epoch 31/56 - Train Loss: 20008.0236 | Val Loss: 171.1454\n",
      "Epoch 32/56 - Train Loss: 166.0187 | Val Loss: 170.9284\n",
      "Epoch 33/56 - Train Loss: 183.7974 | Val Loss: 168.9723\n",
      "Epoch 34/56 - Train Loss: 157.5086 | Val Loss: 169.9414\n",
      "Epoch 35/56 - Train Loss: 175.9446 | Val Loss: 169.2510\n",
      "Epoch 36/56 - Train Loss: 156.2587 | Val Loss: 168.6573\n",
      "Epoch 37/56 - Train Loss: 157.6291 | Val Loss: 169.6100\n",
      "Epoch 38/56 - Train Loss: 154.5291 | Val Loss: 170.0403\n",
      "Epoch 39/56 - Train Loss: 167.1538 | Val Loss: 175.7816\n",
      "Epoch 40/56 - Train Loss: 160.1011 | Val Loss: 172.3033\n",
      "Epoch 41/56 - Train Loss: 6690798821718.3242 | Val Loss: 173.0467\n",
      "Epoch 42/56 - Train Loss: 155.0570 | Val Loss: 174.4509\n",
      "Epoch 43/56 - Train Loss: 158.7858 | Val Loss: 170.8872\n",
      "Epoch 44/56 - Train Loss: 155.3244 | Val Loss: 171.2741\n",
      "Epoch 45/56 - Train Loss: 153.0970 | Val Loss: 178.3512\n",
      "Epoch 46/56 - Train Loss: 164.4159 | Val Loss: 170.5740\n",
      "Epoch 47/56 - Train Loss: 153.5691 | Val Loss: 172.4654\n",
      "Epoch 48/56 - Train Loss: 63508586.4742 | Val Loss: 172.2201\n",
      "Epoch 49/56 - Train Loss: 155.0988 | Val Loss: 169.9392\n",
      "Epoch 50/56 - Train Loss: 152.3621 | Val Loss: 169.3645\n",
      "Epoch 51/56 - Train Loss: 152.1559 | Val Loss: 172.6036\n",
      "Epoch 52/56 - Train Loss: 291.6758 | Val Loss: 172.1506\n",
      "Epoch 53/56 - Train Loss: 150.5188 | Val Loss: 171.7513\n",
      "Epoch 54/56 - Train Loss: 151.9000 | Val Loss: 173.2511\n",
      "Epoch 55/56 - Train Loss: 153.2783 | Val Loss: 173.1519\n",
      "Epoch 56/56 - Train Loss: 150.4633 | Val Loss: 173.0700\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_recommender = VAERecommenderSystem(inter_tensor, latent_dim=64, epochs=56, val_split=0.25)\n",
    "vae_recommender.train()\n",
    "vae_recommender.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('evaluation-data/test_users.txt')\n",
    "user_recommendations = []\n",
    "\n",
    "for id in ids.user_id:\n",
    "    recommendation = vae_recommender.recommend_items(inter_tensor, id, top_k=10)\n",
    "    item_ids = [str(item_id) for item_id, _ in recommendation]\n",
    "    user_recommendations.append([id, ','.join(item_ids)])\n",
    "\n",
    "df = pd.DataFrame(user_recommendations, columns=['user_id', 'recommendations'])\n",
    "\n",
    "df.to_csv('res/rec_vae.tsv', index=False, sep='\\t', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
