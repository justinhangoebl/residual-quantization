{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64  # Latent space dimension\n",
    "NUM_EMBEDDINGS = 128  # Number of vectors in codebook\n",
    "COMMITMENT_COST = 0.25  # Beta in loss function\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming from x -> z_e\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, latent_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)  # No activation, raw latents\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_e -> z_q Codebook Dimension\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Get input shape\n",
    "        x_flattened = x.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "        distances = torch.cdist(x_flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        loss = F.mse_loss(quantized.detach(), x) + self.commitment_cost * F.mse_loss(x.detach(), quantized)\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        \n",
    "        return quantized, loss, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_q -> x_hat Decoding\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(latent_dim, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))  # Output in [0,1]\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim)\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "        self.decoder = Decoder(embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, loss, encoding_indices = self.quantizer(z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "        return x_hat, loss, encoding_indices\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.quantizer(self.encoder(x))[2]\n",
    "    \n",
    "    def decode(self, x):\n",
    "\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:01<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 184.48832999914885\n",
      "Test Loss: 0.36007048026658595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [01:02<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 73.1472650617361\n",
      "Test Loss: 0.28607327153440565\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape () for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m trainer = Trainer(model, optimizer, train_loader, test_loader)\n\u001b[32m     55\u001b[39m trainer.train(\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_reconstructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m trainer.test()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mTrainer.visualize_reconstructions\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m     34\u001b[39m plt.imshow(batch[i].squeeze().detach().numpy(), cmap=\u001b[33m\"\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m plt.subplot(\u001b[32m2\u001b[39m, n, i+\u001b[32m1\u001b[39m+n)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgray\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m plt.subplot(\u001b[32m2\u001b[39m, n, i+\u001b[32m1\u001b[39m+n)\n\u001b[32m     38\u001b[39m plt.imshow(x_hat[i].squeeze().detach().numpy(), cmap=\u001b[33m\"\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:3590\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3568\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3570\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3588\u001b[39m     **kwargs,\n\u001b[32m   3589\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3590\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3594\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3595\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3596\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3600\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3610\u001b[39m     sci(__ret)\n\u001b[32m   3611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5976\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5974\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5976\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5977\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5979\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\matplotlib\\image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\matplotlib\\image.py:653\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    651\u001b[39m     A = A.squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A.shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for image data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[32m    659\u001b[39m     high = \u001b[32m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.issubdtype(A.dtype, np.integer) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Invalid shape () for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, train_loader, test_loader):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch, _ in tqdm(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                x_hat, loss, _ = self.model(batch)\n",
    "                loss_1 = F.binary_cross_entropy(x_hat, batch)\n",
    "                loss = loss.mean()\n",
    "                loss += loss_1\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            self.train_losses.append(train_loss)\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss}\")\n",
    "            self.test()\n",
    "    \n",
    "    def visualize_reconstructions(self, n=5\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        for batch, _ in self.test_loader:\n",
    "            x_hat, _, idx = self.model(batch)\n",
    "            for i in range(n):\n",
    "                plt.subplot(2, n, i+1)\n",
    "                plt.imshow(batch[i].squeeze().detach().numpy(), cmap=\"gray\")\n",
    "                plt.subplot(2, n, i+1+n)\n",
    "                plt.imshow(idx[i].squeeze().detach().numpy(), cmap=\"gray\")\n",
    "                plt.subplot(2, n, i+1+n)\n",
    "                plt.imshow(x_hat[i].squeeze().detach().numpy(), cmap=\"gray\")\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        for batch, _ in self.test_loader:\n",
    "            x_hat, loss, _ = self.model(batch)\n",
    "            loss = loss.mean()\n",
    "            test_loss += loss.item()\n",
    "        self.test_losses.append(test_loss)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "model = VQVAE(NUM_EMBEDDINGS, LATENT_DIM, COMMITMENT_COST)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = Trainer(model, optimizer, train_loader, test_loader)\n",
    "trainer.train(2)\n",
    "trainer.visualize_reconstructions()\n",
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
