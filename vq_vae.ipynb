{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64  # Latent space dimension\n",
    "NUM_EMBEDDINGS = 128  # Number of vectors in codebook\n",
    "COMMITMENT_COST = 0.25  # Beta in loss function\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack(h, num_hiddens, num_residual_layers):\n",
    "    for _ in range(num_residual_layers):\n",
    "        h = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens, num_hiddens, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens, num_hiddens, kernel_size=1, stride=1, padding=0)\n",
    "        )(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming from x -> z_e\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, latent_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)  # No activation, raw latents\n",
    "        residual_stack(x, self.latent_dim, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_e -> z_q Codebook Dimension\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Get input shape\n",
    "        x_flattened = x.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "        distances = torch.cdist(x_flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        loss = F.mse_loss(quantized.detach(), x) + self.commitment_cost * F.mse_loss(x.detach(), quantized)\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        \n",
    "        return quantized, loss, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_q -> x_hat Decoding\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conv1 = nn.ConvTranspose2d(latent_dim, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        residual_stack(x, self.latent_dim, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.sigmoid(self.conv3(x))  # Output in [0,1]\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim)\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "        self.decoder = Decoder(embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, loss, encoding_indices = self.quantizer(z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "        return x_hat, loss, encoding_indices\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.quantizer(self.encoder(x))[2]\n",
    "    \n",
    "    def decode(self, x):\n",
    "\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 22/938 [00:02<01:49,  8.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\u001b[32m     55\u001b[39m trainer = Trainer(model, optimizer, train_loader, test_loader)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m trainer.visualize_reconstructions()\n\u001b[32m     58\u001b[39m trainer.test()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, epochs)\u001b[39m\n\u001b[32m     21\u001b[39m loss = loss.mean()\n\u001b[32m     22\u001b[39m loss += loss_1\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     25\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, train_loader, test_loader):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch, _ in tqdm(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                def add_noise(x):\n",
    "                    return np.clip(x + torch.randn_like(x) * 0.1, 0, 1)\n",
    "                batch = add_noise(batch)\n",
    "                x_hat, loss, _ = self.model(batch)\n",
    "                loss_1 = F.binary_cross_entropy(x_hat, batch)\n",
    "                loss = loss.mean()\n",
    "                loss += loss_1\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            self.train_losses.append(train_loss)\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss}\")\n",
    "            self.test()\n",
    "    \n",
    "    def visualize_reconstructions(self, n=5\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        for batch, _ in self.test_loader:\n",
    "            x_hat, _, idx = self.model(batch)\n",
    "            for i in range(n):\n",
    "                plt.subplot(2, n, i+1)\n",
    "                plt.imshow(batch[i].squeeze().detach().numpy(), cmap=\"gray\")\n",
    "                plt.subplot(2, n, i+1+n)\n",
    "                plt.imshow(x_hat[i].squeeze().detach().numpy(), cmap=\"gray\")\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        for batch, _ in self.test_loader:\n",
    "            x_hat, loss, _ = self.model(batch)\n",
    "            loss = loss.mean()\n",
    "            test_loss += loss.item()\n",
    "        self.test_losses.append(test_loss)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "model = VQVAE(NUM_EMBEDDINGS, LATENT_DIM, COMMITMENT_COST)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = Trainer(model, optimizer, train_loader, test_loader)\n",
    "trainer.train(10)\n",
    "trainer.visualize_reconstructions()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim=16, dropout_prob=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense_layer_1 = nn.Conv1d(num_items, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dense_layer_2 = nn.Conv1d(256, 64, kernel_size=3, stride=1, padding=1)  \n",
    "        self.batch_norm = nn.LayerNorm(64)  \n",
    "        \n",
    "        self.dense_layer_3 = nn.Conv1d(64, embedding_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0)\n",
    "        x = F.leaky_relu(self.dense_layer_1(x).permute(1, 0))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.batch_norm(self.dense_layer_2(x).permute(0, 2, 1)))\n",
    "        x = self.dense_layer_3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim=16, dropout_prob=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense_layer_1 = nn.Conv1d(embedding_dim, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm1 = nn.LayerNorm(64)\n",
    "        \n",
    "        self.dense_layer_2 = nn.Conv1d(64, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm2 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dense_layer_3 = nn.Conv1d(256, num_items, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_layer_1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        x = self.dense_layer_2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dense_layer_3(x)  \n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
    "\n",
    "    def forward(self, z_e):\n",
    "        z_e_reshaped = z_e.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(z_e_reshaped, self.embedding.weight)\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        z_q = self.embedding(encoding_indices).view(z_e.shape)\n",
    "\n",
    "        # Improved VQ loss to encourage embedding diversity\n",
    "        commitment_loss = F.mse_loss(z_q.detach(), z_e)\n",
    "        codebook_loss = F.mse_loss(z_e.detach(), z_q)  # Codebook update loss\n",
    "        loss = commitment_loss + self.commitment_cost * codebook_loss\n",
    "\n",
    "        return z_q, encoding_indices, loss\n",
    "    \n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, num_embeddings):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "        self.decoder = Decoder(input_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, encodings, vq_loss = self.quantizer(z_e)\n",
    "        x_reconstructed = self.decoder(z_q)\n",
    "        return x_reconstructed, vq_loss, encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE_Rec_Sys:\n",
    "    def __init__(self, dataset, num_items, val_split=0.2, embedding_dim=16, num_embeddings=4, batch_size=32):\n",
    "        # Initialize the model components\n",
    "        self.dataset = dataset\n",
    "        self.train_dataset, self.val_dataset = train_test_split(dataset, test_size=val_split, random_state=42)\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.model = VQVAE(input_dim=num_items, latent_dim=embedding_dim, num_embeddings=num_embeddings)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train(self, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0    \n",
    "            progress_bar = tqdm(range(0, len(self.train_dataset), self.batch_size), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "            for i in progress_bar:\n",
    "                # Get the batch from the dataset\n",
    "                batch_data = self.train_dataset[i:i+self.batch_size].detach().to(self.device)\n",
    "                \n",
    "                # Zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through the model\n",
    "                x_reconstructed, vq_loss, _ = self.model(batch_data)\n",
    "\n",
    "                # Compute reconstruction loss\n",
    "                recon_loss = F.binary_cross_entropy(x_reconstructed, batch_data)\n",
    "\n",
    "                # Total loss = reconstruction loss + VQ loss\n",
    "                loss = recon_loss + vq_loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass and optimizer step\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            avg_train_loss = total_loss / len(self.train_dataset)\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation Step\n",
    "            val_loss = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "            # Print the loss for this epoch\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.8f} | Val Loss: {val_loss:.8f}\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(self.val_dataset), self.batch_size):\n",
    "                # Get the batch from the dataset\n",
    "                batch = self.val_dataset[i:i+self.batch_size].detach().to(self.device)\n",
    "                recon_batch, vq_loss, _ = self.model(batch)\n",
    "                loss = F.binary_cross_entropy(recon_batch, batch) + vq_loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        return val_loss / len(self.val_dataset)\n",
    "    \n",
    "    def recommend_items(self, user_index, top_k=10):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Generate the input vector for the user (e.g., user-item interaction vector)\n",
    "        user_data = self.dataset[user_index].detach().unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Forward pass to get reconstructed preferences\n",
    "        with torch.no_grad():\n",
    "            x_reconstructed, _, encodings = self.model(user_data)\n",
    "\n",
    "        # After reconstruction, we use the encoder's output (latent representation)\n",
    "        # Here we just use the reconstructed values to recommend items\n",
    "        # For simplicity, we take the reconstructed output and find the top-k items\n",
    "\n",
    "        reconstructed_preferences = x_reconstructed.squeeze().cpu().numpy()\n",
    "\n",
    "        # Get the top K most recommended items\n",
    "        recommended_items = reconstructed_preferences.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        return recommended_items\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.train_losses[1:], label=\"Train Loss\")\n",
    "        plt.plot(self.val_losses[1:], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4175])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 256, 3], expected input[1, 32, 256] to have 256 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m dataset = torch.FloatTensor(inter_matr)\n\u001b[32m      7\u001b[39m rec_sys = VQVAE_Rec_Sys(dataset, num_items=dataset.shape[\u001b[32m1\u001b[39m], embedding_dim=\u001b[32m16\u001b[39m, num_embeddings=\u001b[32m128\u001b[39m, batch_size=\u001b[32m32\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mrec_sys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m rec_sys.plot_loss()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mVQVAE_Rec_Sys.train\u001b[39m\u001b[34m(self, epochs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m x_reconstructed, vq_loss, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Compute reconstruction loss\u001b[39;00m\n\u001b[32m     34\u001b[39m recon_loss = F.binary_cross_entropy(x_reconstructed, batch_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mVQVAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mprint\u001b[39m(x.shape)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     z_e = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     z_q, encodings, vq_loss = \u001b[38;5;28mself\u001b[39m.quantizer(z_e)\n\u001b[32m     83\u001b[39m     x_reconstructed = \u001b[38;5;28mself\u001b[39m.decoder(z_q)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m x = F.leaky_relu(\u001b[38;5;28mself\u001b[39m.dense_layer_1(x).permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m     15\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m x = F.leaky_relu(\u001b[38;5;28mself\u001b[39m.batch_norm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense_layer_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)))\n\u001b[32m     17\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dense_layer_3(x)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hango\\OneDrive\\Dokumente\\MasterThesis\\residual-quantization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    360\u001b[39m         F.pad(\n\u001b[32m    361\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    369\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [64, 256, 3], expected input[1, 32, 256] to have 256 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/lfm_interactions.csv', sep=\"\\t\", index_col=0)\n",
    "inter_matr = pd.pivot_table(df, values='count', index='user_id', columns='item_id')\n",
    "inter_matr = inter_matr.fillna(0).to_numpy()\n",
    "inter_matr = (inter_matr > 0).astype(int)\n",
    "\n",
    "dataset = torch.FloatTensor(inter_matr)\n",
    "rec_sys = VQVAE_Rec_Sys(dataset, num_items=dataset.shape[1], embedding_dim=16, num_embeddings=128, batch_size=32)\n",
    "rec_sys.train(epochs=40)\n",
    "\n",
    "rec_sys.plot_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv('evaluation-data/test_users.txt')\n",
    "user_recommendations = []\n",
    "print(rec_sys.dataset.shape[0])\n",
    "\n",
    "\n",
    "for id in ids.user_id:\n",
    "    recommendation = rec_sys.recommend_items(id, top_k=10)\n",
    "    user_recommendations.append([id, ','.join(recommendation.astype(str))])\n",
    "\n",
    "df = pd.DataFrame(user_recommendations, columns=['user_id', 'recommendations'])\n",
    "\n",
    "df.to_csv('res/rec_vq-vae.tsv', index=False, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Compression using VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim=16, dropout_prob=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_items, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim=16, dropout_prob=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, num_items)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.mlp(x))\n",
    "    \n",
    "class Quanitzation(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super(Quanitzation, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
    "        \n",
    "    def forward(self, z_e):\n",
    "        z_e_reshaped = z_e.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(z_e_reshaped, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        z_q = self.embedding(encoding_indices).view(z_e.shape)\n",
    "        \n",
    "        commitment_loss = F.mse_loss(z_q.detach(), z_e)\n",
    "        codebook_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        loss = commitment_loss + self.commitment_cost * codebook_loss\n",
    "        \n",
    "        return z_q, encoding_indices, loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
